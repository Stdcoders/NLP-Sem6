{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# NLP Assignment 2 - To perform bag-of-words approach and create word embedding for text data"
      ],
      "metadata": {
        "id": "s28P52BxYLkX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8TBA3HjWXIK",
        "outputId": "e257e5a0-f593-4992-b5a6-dfbf78f70511"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n",
            "Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (27.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m37.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gensim\n",
            "Successfully installed gensim-4.4.0\n"
          ]
        }
      ],
      "source": [
        "!pip install gensim nltk scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bjOs8VnMWtPj",
        "outputId": "8c3258c0-a758-44ae-8367-a95b8bfa037d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [\n",
        "    \"This is an example of natural language processing\",\n",
        "    \"Tokenization is the process of breaking down words into different tokens\",\n",
        "    \"Tokenization is essential for creating embeddings and understanding the context for the computer\"\n",
        "]"
      ],
      "metadata": {
        "id": "_ULhn-aCW19M"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Count Vectorizer (Bag Of Words Approach)"
      ],
      "metadata": {
        "id": "BIFPANUrYbSe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "count_vectorizer = CountVectorizer()\n",
        "bow_counts = count_vectorizer.fit_transform(sentences)\n",
        "\n",
        "print(\"Vocabulary:\")\n",
        "print(count_vectorizer.get_feature_names_out())\n",
        "\n",
        "print(\"\\nBag of Words (Count Matrix):\")\n",
        "print(bow_counts.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tn1RnddsXLw2",
        "outputId": "0e50be80-085e-4743-c0d5-892c10358d17"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary:\n",
            "['an' 'and' 'breaking' 'computer' 'context' 'creating' 'different' 'down'\n",
            " 'embeddings' 'essential' 'example' 'for' 'into' 'is' 'language' 'natural'\n",
            " 'of' 'process' 'processing' 'the' 'this' 'tokenization' 'tokens'\n",
            " 'understanding' 'words']\n",
            "\n",
            "Bag of Words (Count Matrix):\n",
            "[[1 0 0 0 0 0 0 0 0 0 1 0 0 1 1 1 1 0 1 0 1 0 0 0 0]\n",
            " [0 0 1 0 0 0 1 1 0 0 0 0 1 1 0 0 1 1 0 1 0 1 1 0 1]\n",
            " [0 1 0 1 1 1 0 0 1 1 0 2 0 1 0 0 0 0 0 2 0 1 0 1 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Normalized Bag of Words Matrix"
      ],
      "metadata": {
        "id": "FC4ZGIQPZA4c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bow_array = bow_counts.toarray().astype(float)\n",
        "\n",
        "normalized_bow = bow_array / bow_array.sum(axis=1, keepdims=True)\n",
        "\n",
        "print(\"Normalized Bag of Words:\")\n",
        "print(normalized_bow)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0TZLQel5XOqE",
        "outputId": "c4ca9403-0705-48c6-b618-f47f650930d4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Normalized Bag of Words:\n",
            "[[0.125      0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.125      0.\n",
            "  0.         0.125      0.125      0.125      0.125      0.\n",
            "  0.125      0.         0.125      0.         0.         0.\n",
            "  0.        ]\n",
            " [0.         0.         0.09090909 0.         0.         0.\n",
            "  0.09090909 0.09090909 0.         0.         0.         0.\n",
            "  0.09090909 0.09090909 0.         0.         0.09090909 0.09090909\n",
            "  0.         0.09090909 0.         0.09090909 0.09090909 0.\n",
            "  0.09090909]\n",
            " [0.         0.07692308 0.         0.07692308 0.07692308 0.07692308\n",
            "  0.         0.         0.07692308 0.07692308 0.         0.15384615\n",
            "  0.         0.07692308 0.         0.         0.         0.\n",
            "  0.         0.15384615 0.         0.07692308 0.         0.07692308\n",
            "  0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. TF-IDF Vectorizer"
      ],
      "metadata": {
        "id": "gM2SDxBtZHaF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(sentences)\n",
        "\n",
        "print(\"TF-IDF Vocabulary:\")\n",
        "print(tfidf_vectorizer.get_feature_names_out())\n",
        "\n",
        "print(\"\\nTF-IDF Matrix:\")\n",
        "print(tfidf_matrix.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iH-wYfROXU0C",
        "outputId": "d26c44f5-96e0-408b-8298-dabb32c969d4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF Vocabulary:\n",
            "['an' 'and' 'breaking' 'computer' 'context' 'creating' 'different' 'down'\n",
            " 'embeddings' 'essential' 'example' 'for' 'into' 'is' 'language' 'natural'\n",
            " 'of' 'process' 'processing' 'the' 'this' 'tokenization' 'tokens'\n",
            " 'understanding' 'words']\n",
            "\n",
            "TF-IDF Matrix:\n",
            "[[0.37994462 0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.37994462 0.\n",
            "  0.         0.22440141 0.37994462 0.37994462 0.28895767 0.\n",
            "  0.37994462 0.         0.37994462 0.         0.         0.\n",
            "  0.        ]\n",
            " [0.         0.         0.33178811 0.         0.         0.\n",
            "  0.33178811 0.33178811 0.         0.         0.         0.\n",
            "  0.33178811 0.1959594  0.         0.         0.25233341 0.33178811\n",
            "  0.         0.25233341 0.         0.25233341 0.33178811 0.\n",
            "  0.33178811]\n",
            " [0.         0.2649918  0.         0.2649918  0.2649918  0.2649918\n",
            "  0.         0.         0.2649918  0.2649918  0.         0.52998359\n",
            "  0.         0.15650842 0.         0.         0.         0.\n",
            "  0.         0.40306618 0.         0.20153309 0.         0.2649918\n",
            "  0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Tokenization using nltk - punkt tab"
      ],
      "metadata": {
        "id": "UKLZj0ROZNhc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "tokenized_sentences = [word_tokenize(sentence.lower()) for sentence in sentences]\n",
        "\n",
        "print(tokenized_sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "STRGEmELXaDd",
        "outputId": "6c85859e-e2f5-4d51-c746-10bc22fe9d03"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['this', 'is', 'an', 'example', 'of', 'natural', 'language', 'processing'], ['tokenization', 'is', 'the', 'process', 'of', 'breaking', 'down', 'words', 'into', 'different', 'tokens'], ['tokenization', 'is', 'essential', 'for', 'creating', 'embeddings', 'and', 'understanding', 'the', 'context', 'for', 'the', 'computer']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Training Word-2-Vec Model"
      ],
      "metadata": {
        "id": "T6XdANxJZSFz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "w2v_model = Word2Vec(\n",
        "    sentences=tokenized_sentences,\n",
        "    vector_size=100,\n",
        "    window=5,\n",
        "    min_count=1,\n",
        "    workers=4\n",
        ")"
      ],
      "metadata": {
        "id": "3ZQKQ9EHXxp3"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Embedding for word 'language':\")\n",
        "print(w2v_model.wv['language'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KA1NpIURX8bV",
        "outputId": "0c6eb02a-9b90-4af0-a1ce-dedfa2cbba4e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding for word 'language':\n",
            "[ 0.00257044  0.00084246 -0.00254006  0.00936029  0.00275908  0.00409472\n",
            " -0.0011834   0.0009081   0.00662517 -0.00072785  0.00334232 -0.00067519\n",
            "  0.00524809  0.00364064  0.00257861 -0.00530791 -0.00471027  0.00430493\n",
            " -0.00590659 -0.00018213 -0.00063406  0.00349599 -0.00844565  0.008818\n",
            " -0.00144616 -0.00533231  0.0040541  -0.00193368 -0.00776703 -0.00449852\n",
            " -0.00038823 -0.00894646  0.00057242  0.00244541 -0.00322876  0.00257358\n",
            "  0.00248184  0.00998698  0.00143067  0.00201974  0.00277918 -0.0020759\n",
            " -0.00870117  0.00802288 -0.00197471 -0.00969043 -0.00655291 -0.00394493\n",
            "  0.00395548  0.00504247  0.00608555 -0.00676943  0.00068828 -0.00277378\n",
            " -0.00520646  0.0069785   0.00394911 -0.0031087  -0.00827575 -0.00514036\n",
            " -0.00065048  0.00781425  0.00604725 -0.00845304 -0.00956594  0.00713241\n",
            " -0.00233162 -0.00369382  0.00574952 -0.00584621  0.0050972  -0.0002393\n",
            " -0.0068743  -0.00033539  0.0063573   0.00929013  0.00222292  0.00504594\n",
            " -0.0049726  -0.0007973  -0.00531616  0.00118862 -0.00179621 -0.00363151\n",
            " -0.0070151   0.0096532   0.00297214 -0.0022832  -0.00418146  0.00771532\n",
            " -0.00648464  0.00312505  0.00079035  0.00832716  0.00683948 -0.00290956\n",
            "  0.0025289  -0.00166431 -0.00945168 -0.00261383]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Sentence Embeddings"
      ],
      "metadata": {
        "id": "6KQ-OFyGZa_r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sentence_embedding(sentence, model):\n",
        "    words = word_tokenize(sentence.lower())\n",
        "    vectors = [model.wv[word] for word in words if word in model.wv]\n",
        "    return np.mean(vectors, axis=0)\n",
        "\n",
        "print(\"Sentence Embedding:\")\n",
        "print(sentence_embedding(sentences[0], w2v_model))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nnt18t7ZX_CF",
        "outputId": "0b553f5e-39e9-4c63-a037-e661b38262e5"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence Embedding:\n",
            "[-1.7456166e-03  1.8554007e-03  2.5204441e-03  2.9984203e-03\n",
            " -2.6371318e-03  3.5544392e-05  5.1276397e-04  1.4629253e-03\n",
            " -5.5768737e-04 -2.1241603e-03 -1.0773984e-03 -1.5179085e-04\n",
            "  1.6350774e-03  2.7995061e-03  1.1911661e-03  1.2271837e-03\n",
            "  2.2056834e-03  1.1445445e-03 -6.3199922e-04 -1.4179916e-03\n",
            "  7.3126139e-05 -3.3079641e-04 -1.9750257e-03  1.3939199e-03\n",
            " -1.5637996e-03  2.6480285e-03 -1.7499253e-03  3.4582103e-04\n",
            "  9.6437754e-04  6.4160209e-04  4.0156916e-03 -1.1038890e-03\n",
            " -5.6187366e-04 -3.4021945e-03  1.2099890e-03 -1.0483969e-03\n",
            "  2.7857623e-03 -6.0807954e-04 -4.9243827e-04  2.8416496e-03\n",
            "  2.8031536e-03 -8.3858310e-04 -1.5831259e-03  4.4402189e-04\n",
            " -3.4977711e-04 -4.0636589e-03  8.0804812e-04 -2.3284815e-03\n",
            "  1.0986961e-03 -8.3142734e-04  4.2281114e-03 -5.8895443e-04\n",
            " -1.0770948e-03  1.2687740e-03 -9.7300851e-04  4.0142341e-03\n",
            "  2.2083516e-03  9.7076758e-04  4.7949149e-04 -9.7748148e-04\n",
            " -1.5888590e-03  1.1717086e-04 -4.1859673e-04  1.3065455e-03\n",
            " -1.8357020e-04  3.4766733e-03  1.0307115e-03 -2.4786650e-03\n",
            " -1.2896152e-04  4.1179731e-03 -1.9666825e-03  3.5879191e-04\n",
            " -5.1207177e-04 -1.3304928e-03  1.0265319e-03  1.4603623e-03\n",
            "  1.9381821e-04  3.5304907e-03  1.5047432e-03  3.1766499e-04\n",
            " -4.7647087e-03 -5.3427718e-04  2.6170937e-03 -6.8816007e-04\n",
            " -1.4628337e-03  7.8332680e-04  1.0124303e-03 -4.7040154e-04\n",
            " -1.4220926e-03  1.3049345e-03  1.6426222e-03 -3.2928071e-03\n",
            " -1.7065792e-03 -2.3998285e-04 -1.0057180e-03 -3.8044346e-03\n",
            "  2.3767180e-03  1.5504675e-03 -1.5127800e-03  3.3356424e-03]\n"
          ]
        }
      ]
    }
  ]
}