{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# NLP Assignment 1 - To perform tokenization, stemming, lemmatization"
      ],
      "metadata": {
        "id": "WIXQrKUGu4oC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9HDh1hY-mofy",
        "outputId": "1082129c-ba19-416c-f844-21ad0ec7603b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: NLTK in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from NLTK) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from NLTK) (1.5.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from NLTK) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from NLTK) (4.67.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install NLTK"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk"
      ],
      "metadata": {
        "id": "Du2R31PvsFXZ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Natural Language Processing is a domain of artificial intelligence, which deals with helping machines learn human language and communicate with them.\""
      ],
      "metadata": {
        "id": "1XrkxorCsKkf"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0NwL-goYs43W",
        "outputId": "df355898-96ab-433b-c28b-16619ee892d3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. White Space Tokenization"
      ],
      "metadata": {
        "id": "n4Kgzgb8uDNN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ws = nltk.WhitespaceTokenizer()\n",
        "print(ws.tokenize(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6rYCXRcqt_q3",
        "outputId": "36f59f00-7708-447f-f8b9-cd16da9c2711"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Natural', 'Language', 'Processing', 'is', 'a', 'domain', 'of', 'artificial', 'intelligence,', 'which', 'deals', 'with', 'helping', 'machines', 'learn', 'human', 'language', 'and', 'communicate', 'with', 'them.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Punctuation based tokenization"
      ],
      "metadata": {
        "id": "CkeSSSKDuwzH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(nltk.wordpunct_tokenize(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nxJ2t1z5uQkw",
        "outputId": "25eac694-5393-40c0-b4b2-24779b3d89be"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Natural', 'Language', 'Processing', 'is', 'a', 'domain', 'of', 'artificial', 'intelligence', ',', 'which', 'deals', 'with', 'helping', 'machines', 'learn', 'human', 'language', 'and', 'communicate', 'with', 'them', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Tweet Based Tokenization"
      ],
      "metadata": {
        "id": "HrGMeWoHvD2P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TweetTokenizer\n",
        "\n",
        "tt = TweetTokenizer()\n",
        "print(tt.tokenize(text))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "grg3kUV3vZXV",
        "outputId": "a4889695-1fca-4f20-c6a1-ed9efc14756a"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Natural', 'Language', 'Processing', 'is', 'a', 'domain', 'of', 'artificial', 'intelligence', ',', 'which', 'deals', 'with', 'helping', 'machines', 'learn', 'human', 'language', 'and', 'communicate', 'with', 'them', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Tree Bank Based Tokenization"
      ],
      "metadata": {
        "id": "JVFGje2Mvjgy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "\n",
        "tb = TreebankWordTokenizer()\n",
        "print(tb.tokenize(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3X4K0WywviPn",
        "outputId": "70cd2051-ddb0-43ff-d88f-b3a81a70369b"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Natural', 'Language', 'Processing', 'is', 'a', 'domain', 'of', 'artificial', 'intelligence', ',', 'which', 'deals', 'with', 'helping', 'machines', 'learn', 'human', 'language', 'and', 'communicate', 'with', 'them', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. MWE Based Tokenization"
      ],
      "metadata": {
        "id": "q-GkFejwwFTo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import MWETokenizer\n",
        "\n",
        "mwe = MWETokenizer([('Machine', 'Learning')], separator='_')\n",
        "print(mwe.tokenize(\"I love Machine Learning and NLP\".split()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jHLO-faIvpE0",
        "outputId": "eb9441c9-a454-4a2b-855f-5ce0cdc6b186"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'love', 'Machine_Learning', 'and', 'NLP']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Stemming Techniques - Porter and Snow Ball"
      ],
      "metadata": {
        "id": "PrfZp-wBw80n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Porter technique\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "ps = PorterStemmer()\n",
        "words = ['running', 'runner', 'runs', 'easily']\n",
        "print([ps.stem(w) for w in words])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IaHzmuFzwENl",
        "outputId": "349953ea-4aa3-498d-9fa5-85122ca5bce2"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['run', 'runner', 'run', 'easili']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Snowball technique\n",
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "ss = SnowballStemmer(\"english\")\n",
        "print([ss.stem(w) for w in words])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dwFX4Fa2xKWD",
        "outputId": "3ed1bb1e-91e4-4a19-c576-11ecd8563b0e"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['run', 'runner', 'run', 'easili']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Lemmatization using Word Net"
      ],
      "metadata": {
        "id": "6H9SkAhPxWi5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "print(lemmatizer.lemmatize(\"running\", pos='v'))\n",
        "print(lemmatizer.lemmatize(\"better\", pos='a'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e_pb-rHQxP7D",
        "outputId": "82e7a103-5430-4b79-b97d-f9c62779e0a2"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "run\n",
            "good\n"
          ]
        }
      ]
    }
  ]
}